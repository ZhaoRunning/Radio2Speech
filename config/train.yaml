# Transunet parameters
hidden_size: 768
transformer_num_layers: 12
mlp_dim: 3072
num_heads: 12
transformer_dropout_rate: 0.1
transformer_attention_dropout_rate: 0.0
transformer_pretrained_path: /your/path/R50+ViT-B_16.npz

# visualization
disp_iter: 50

# training parameters
batch_size: 60
val_batch_size: 50
learning_rate: 0.0025
epochs: 150
opt: "adam"
lr_scheduler: "warmup"
warmup_ratio: 0.2
step_size: 200

distributed: True
local_rank: 0

save_freq: 20
metrics_every: 30

